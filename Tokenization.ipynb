{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "paragraph = ('''Speech technology is based on machine learning, deep learning and statistical models.To develop system for new language we need some considerable resources. Multi-lingual\n",
    "acoustic modeling using Subspace Gaussian Mixture Model for low-resource languages of Indian origin is investigated. We will build acoustic model for a low-resource lan-\n",
    "guage with limited vocabulary by leveraging resources from another language with comparatively larger resources was focused upon. Experiments will be doing on Hindi\n",
    "and Marathi corpus from MANDI database, with Hindi having greater resources than Marathi. The development of models which can factor language-dependent and language-\n",
    "independent aspects of the speech signal, perhaps exploiting in-variance derived from speech production''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Speech technology is based on machine learning, deep learning and statistical models.To develop system for new language we need some considerable resources.', 'Multi-lingual\\nacoustic modeling using Subspace Gaussian Mixture Model for low-resource languages of Indian origin is investigated.', 'We will build acoustic model for a low-resource lan-\\nguage with limited vocabulary by leveraging resources from another language with comparatively larger resources was focused upon.', 'Experiments will be doing on Hindi\\nand Marathi corpus from MANDI database, with Hindi having greater resources than Marathi.', 'The development of models which can factor language-dependent and language-\\nindependent aspects of the speech signal, perhaps exploiting in-variance derived from speech production']\n"
     ]
    }
   ],
   "source": [
    "sentence = nltk.sent_tokenize(paragraph) # tokenize into sentences\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = nltk.word_tokenize(paragraph)  # tokenize into words\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the develop model factor language-depend language- independ aspect speech signal , perhap exploit in-vari deriv speech product\n"
     ]
    }
   ],
   "source": [
    "#Stemming (#word meaning is not important)#\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords # words like am,i,me, are,the, of the, of them... like this word we remove using stopword\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "stemmer = PorterStemmer() # create obj..\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ' '.join(words)\n",
    "print(sentence[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development model factor language-dependent language- independent aspect speech signal , perhaps exploiting in-variance derived speech production\n"
     ]
    }
   ],
   "source": [
    "#lemmatization is better than  stemming for good word meaning\n",
    "from nltk.stem import WordNetLemmatizer # import lemmatization lib\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ' '.join(words)\n",
    "print(sentence[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Word(Document Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Word(Document Matrix)#\n",
    "#preprocessing of paragraph\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development model factor language dependent language independent aspect speech signal perhaps exploiting variance derived speech production\n"
     ]
    }
   ],
   "source": [
    "PS = PorterStemmer()\n",
    "WNL =WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    #Stemming \n",
    "    #text = [PS.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    #lemmatizing\n",
    "    text = [WNL.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    text = ' '.join(text)\n",
    "    corpus.append(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 1 0 0\n",
      "  0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0\n",
      "  1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 2\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 1 1 0 1 2 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#Bag of Word model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.25315956 0.         0.\n",
      "  0.25315956 0.         0.         0.25315956 0.         0.\n",
      "  0.25315956 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14262568 0.         0.50631912 0.\n",
      "  0.         0.         0.         0.25315956 0.         0.\n",
      "  0.         0.14262568 0.         0.         0.25315956 0.25315956\n",
      "  0.         0.         0.         0.14262568 0.         0.20424754\n",
      "  0.25315956 0.         0.25315956 0.25315956 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.23047456 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28566728 0.         0.         0.         0.         0.28566728\n",
      "  0.28566728 0.         0.16093996 0.         0.         0.\n",
      "  0.         0.28566728 0.23047456 0.         0.         0.\n",
      "  0.28566728 0.16093996 0.28566728 0.28566728 0.         0.\n",
      "  0.28566728 0.         0.         0.16093996 0.         0.\n",
      "  0.         0.28566728 0.         0.         0.         0.28566728\n",
      "  0.         0.        ]\n",
      " [0.20301448 0.25163121 0.         0.         0.25163121 0.25163121\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25163121\n",
      "  0.         0.         0.25163121 0.         0.         0.\n",
      "  0.         0.25163121 0.14176463 0.25163121 0.         0.25163121\n",
      "  0.25163121 0.         0.20301448 0.         0.         0.\n",
      "  0.         0.14176463 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4252939  0.         0.\n",
      "  0.         0.         0.         0.         0.25163121 0.\n",
      "  0.         0.25163121]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27402506 0.27402506 0.         0.         0.\n",
      "  0.         0.         0.27402506 0.         0.         0.\n",
      "  0.         0.27402506 0.         0.54805012 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.27402506 0.54805012\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.15438094 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2565734  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2565734  0.2565734\n",
      "  0.         0.2565734  0.         0.2565734  0.2565734  0.\n",
      "  0.         0.         0.         0.         0.2565734  0.\n",
      "  0.         0.         0.28909795 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14454898 0.         0.         0.         0.\n",
      "  0.         0.2565734  0.2565734  0.         0.2565734  0.4140036\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2565734  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
