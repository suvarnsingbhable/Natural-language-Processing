{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of paragraph\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "paragraph = ('''Speech technology is based on machine learning, deep learning and statistical models.To develop system for new language we need some considerable resources. Multi-lingual\n",
    "acoustic modeling using Subspace Gaussian Mixture Model for low-resource languages of Indian origin is investigated. We will build acoustic model for a low-resource lan-\n",
    "guage with limited vocabulary by leveraging resources from another language with comparatively larger resources was focused upon. Experiments will be doing on Hindi\n",
    "and Marathi corpus from MANDI database, with Hindi having greater resources than Marathi. The development of models which can factor language-dependent and language-\n",
    "independent aspects of the speech signal, perhaps exploiting in-variance derived from speech production''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "develop model factor languag depend languag independ aspect speech signal perhap exploit varianc deriv speech product\n"
     ]
    }
   ],
   "source": [
    "PS = PorterStemmer()\n",
    "WNL =WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    #Stemming \n",
    "    text = [PS.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    #lemmatizing\n",
    "    text = [WNL.lemma(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    corpus.append(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 1 0 0 0\n",
      "  1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1\n",
      "  2 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
      "  1 0 0 0 0 0 0 3 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 1 1 0 1 2 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#Bag of Word model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
